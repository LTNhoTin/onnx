# Triá»ƒn khai Model Super Resolution trÃªn Triton

## 1 Chuáº©n bá»‹ MÃ´ hÃ¬nh Super Resolution
### MÃ´ hÃ¬nh
- Model Super Resolution Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trong **Training 1**.
- Chuyá»ƒn Ä‘á»•i tá»« **PyTorch â†’ ONNX** Ä‘á»ƒ cháº¡y trÃªn Triton.

### Xuáº¥t Model tá»« PyTorch sang ONNX
1. Táº¡o model Super Resolution vÃ  táº£i trá»ng sá»‘ Ä‘Ã£ huáº¥n luyá»‡n trÆ°á»›c.
2. Xuáº¥t model sang ONNX vá»›i dynamic batch:
   ```python
   torch.onnx.export(model, x, "super_resolution.onnx", export_params=True, opset_version=10,
                     do_constant_folding=True, input_names=['input'], output_names=['output'],
                     dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})
   ```
3. Kiá»ƒm tra láº¡i model ONNX báº±ng ONNX Runtime:
   ```python
   import onnx
   onnx_model = onnx.load("super_resolution.onnx")
   onnx.checker.check_model(onnx_model)
   ```

---

## 2 Triá»ƒn khai Model trÃªn Triton
### CÃ i Ä‘áº·t Triton Server
1. Táº£i Triton Server tá»« NVIDIA Container Registry:
   ```sh
   docker pull --platform linux/amd64 nvcr.io/nvidia/tritonserver:25.01-py3
   ```

2. Táº¡o **model repository** vÃ  Ä‘áº·t model vÃ o Ä‘Ãºng thÆ° má»¥c:
   ```sh
   mkdir -p model_repository/super_resolution/1
   mv super_resolution.onnx model_repository/super_resolution/1/
   ```

3. **Táº¡o file `config.pbtxt`** Ä‘á»ƒ khai bÃ¡o model trong Triton.

---

### Cháº¡y Triton Server
```sh
docker run --rm -p 8000:8000 -p 8001:8001 -p 8002:8002     -v $(pwd)/model_repository:/models     --platform linux/amd64 nvcr.io/nvidia/tritonserver:25.01-py3     tritonserver --model-repository=/models
```
**Náº¿u server cháº¡y thÃ nh cÃ´ng**, báº¡n sáº½ tháº¥y log:
```
+------------------+------+
| Model            | Version | Status |
+------------------+------+
| super_resolution | 1       | READY  |
```
**Model Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ cháº¡y inference!**

---

## 3 Kiá»ƒm tra Inference vá»›i Triton
### CÃ i Ä‘áº·t Triton Client
```sh
pip install tritonclient[http]
```

### Gá»­i request inference Ä‘áº¿n Triton
Cháº¡y script `inference.py` Ä‘á»ƒ test inference:
```sh
python3 inference.py
```

### Kiá»ƒm tra káº¿t quáº£
- áº¢nh Ä‘áº§u vÃ o: **`test.jpg`**
- áº¢nh Ä‘áº§u ra Ä‘Æ°á»£c lÆ°u: **`output_super_resolution.jpg`**
- Náº¿u inference thÃ nh cÃ´ng, áº£nh Ä‘Æ°á»£c phÃ³ng to sáº½ hiá»ƒn thá»‹.

---

## 4 Äo Hiá»‡u Suáº¥t báº±ng Perf Analyzer
### Táº£i & Cháº¡y Triton SDK
1. Táº£i Triton SDK:
   ```sh
   docker pull --platform linux/amd64 nvcr.io/nvidia/tritonserver:25.01-py3-sdk
   ```
2. Cháº¡y SDK Ä‘á»ƒ Ä‘o hiá»‡u suáº¥t:
   ```sh
   docker run --rm -it --net host --platform linux/amd64 nvcr.io/nvidia/tritonserver:25.01-py3-sdk
   ```

---

### Äo hiá»‡u suáº¥t vá»›i Perf Analyzer
1. **Batch size = 1**:
   ```sh
   perf_analyzer -m super_resolution
   ```
2. **Batch size = 8**:
   ```sh
   perf_analyzer -m super_resolution -b 8
   ```
3. **Thá»­ nghiá»‡m vá»›i nhiá»u concurrency (1-11)**:
   ```sh
   perf_analyzer -m super_resolution --concurrency-range 1:11:2
   ```
ğŸ”¹ **ThÃ´ng sá»‘ quan trá»ng Ä‘Æ°á»£c Ä‘o**:
- **Throughput** (áº£nh/giÃ¢y)
- **Latency** (thá»i gian xá»­ lÃ½ má»—i request)
- **áº¢nh hÆ°á»Ÿng cá»§a batch size & concurrency Ä‘áº¿n hiá»‡u suáº¥t**

**Sau khi Ä‘o xong, tá»•ng há»£p káº¿t quáº£ vÃ o bÃ¡o cÃ¡o `report.md`.**